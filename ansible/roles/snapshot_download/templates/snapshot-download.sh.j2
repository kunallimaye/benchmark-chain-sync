#!/bin/bash
# =============================================================================
# Snapshot Download Script
# =============================================================================
# Downloads and extracts Base snapshot for op-reth.
#
# Two modes:
#   - Streaming (LSSD): curl | pv | zstd | tar (single connection, no temp file)
#   - Standard (PD):    aria2c download + separate extraction (faster, resumable)
#
# Writes progress to status file for monitoring.
# =============================================================================

set -euo pipefail

# Configuration
STATUS_FILE="{{ snapshot_status_file }}"
SNAPSHOT_URL="{{ snapshot_url }}"
DATA_DIR="{{ op_reth_data_dir }}"
TEMP_DIR="{{ snapshot_temp_dir }}"
SNAPSHOT_FILE="$TEMP_DIR/snapshot.tar.zst"
STREAMING_MODE="{{ streaming_extraction | lower }}"

# Write status to JSON file
write_status() {
    local stage="$1"
    local progress="$2"
    local error="${3:-}"
    
    if [ -n "$error" ]; then
        echo "{\"stage\": \"$stage\", \"progress\": \"$progress\", \"error\": \"$error\", \"timestamp\": \"$(date -Iseconds)\"}" > "$STATUS_FILE"
    else
        echo "{\"stage\": \"$stage\", \"progress\": \"$progress\", \"timestamp\": \"$(date -Iseconds)\"}" > "$STATUS_FILE"
    fi
}

# Error handler
on_error() {
    local exit_code=$?
    write_status "failed" "unknown" "Script exited with code $exit_code"
    exit $exit_code
}
trap on_error ERR

echo "=== Starting Snapshot Download ==="
echo "URL: $SNAPSHOT_URL"
echo "Data directory: $DATA_DIR"
echo "Streaming mode: $STREAMING_MODE"

# Check if already complete
if [ -f "$STATUS_FILE" ] && grep -q '"stage": "ready"' "$STATUS_FILE" 2>/dev/null; then
    echo "Configuration already complete, skipping download"
    exit 0
fi

# Check if snapshot data already exists
if [ -d "$DATA_DIR/db" ] && [ "$(ls -A $DATA_DIR/db 2>/dev/null)" ]; then
    echo "Snapshot data already exists at $DATA_DIR/db, skipping download"
    write_status "complete" "100%"
    # Trigger configure-finish (--no-block to avoid deadlock with After= dependency)
    systemctl daemon-reload
    systemctl start configure-finish.service --no-block || true
    exit 0
fi

# Create directories
mkdir -p "$TEMP_DIR" "$DATA_DIR"
chown {{ service_user }}:{{ service_group }} "$DATA_DIR"

# Generate signed URL for GCS access
write_status "signing" "generating signed URL"
echo "=== Generating Signed URL ==="

# Get service account email from metadata
SA_EMAIL=$(curl -s -H "Metadata-Flavor: Google" \
    "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email")

if [ -z "$SA_EMAIL" ]; then
    write_status "failed" "sign-url" "Failed to get service account email from metadata"
    exit 1
fi

echo "Service account: $SA_EMAIL"
echo "Generating signed URL (valid for 12 hours)..."

SIGNED_URL=$(gcloud storage sign-url "$SNAPSHOT_URL" \
    --impersonate-service-account="$SA_EMAIL" \
    --duration=12h \
    --region=us-central1 \
    --format="value(signed_url)" 2>&1)

if [ -z "$SIGNED_URL" ] || [[ "$SIGNED_URL" == *"ERROR"* ]]; then
    write_status "failed" "sign-url" "Failed to generate signed URL: $SIGNED_URL"
    exit 1
fi

echo "Signed URL generated successfully"

# Create extraction directory
EXTRACT_DIR="$DATA_DIR/.extract_temp"
mkdir -p "$EXTRACT_DIR"

# =============================================================================
# Streaming Mode (LSSD) - curl | pv | zstd | tar
# =============================================================================
if [ "$STREAMING_MODE" = "true" ]; then
    echo "=== Streaming extraction mode (no temp file) ==="
    write_status "streaming" "0%"
    
    # Get total size from HTTP header for progress calculation
    echo "Getting file size..."
    TOTAL_SIZE=$(curl -sI "$SIGNED_URL" | grep -i "^content-length:" | head -1 | awk '{print $2}' | tr -d '\r\n' || echo "0")
    echo "Total size: $TOTAL_SIZE bytes"
    
    if [ "$TOTAL_SIZE" = "0" ] || [ -z "$TOTAL_SIZE" ]; then
        echo "WARNING: Could not determine file size, progress will be estimated"
        TOTAL_SIZE="5000000000000"  # ~5TB estimate
    fi
    
    # Create progress tracking file
    PV_PROGRESS_FILE="/tmp/pv_progress"
    rm -f "$PV_PROGRESS_FILE"
    
    # Start streaming extraction in background
    # pv -n outputs numeric progress (0-100) to stderr
    echo "=== Starting streaming download and extraction ==="
    (
        curl -sL "$SIGNED_URL" 2>/dev/null | \
            pv -n -s "$TOTAL_SIZE" 2>"$PV_PROGRESS_FILE" | \
            zstd -d 2>/dev/null | \
            tar -xf - -C "$EXTRACT_DIR" 2>&1
    ) &
    STREAM_PID=$!
    
    # Monitor progress
    echo "Monitoring progress (PID: $STREAM_PID)..."
    while kill -0 $STREAM_PID 2>/dev/null; do
        if [ -f "$PV_PROGRESS_FILE" ]; then
            # pv -n outputs lines like: 0, 1, 2, ... 100
            PROGRESS=$(tail -1 "$PV_PROGRESS_FILE" 2>/dev/null || echo "0")
            if [ -n "$PROGRESS" ] && [ "$PROGRESS" != "0" ]; then
                write_status "streaming" "${PROGRESS}%"
                echo "Progress: ${PROGRESS}%"
            fi
        fi
        sleep 10
    done
    
    # Wait for streaming to finish and get exit code
    wait $STREAM_PID
    STREAM_EXIT=$?
    
    if [ $STREAM_EXIT -ne 0 ]; then
        write_status "failed" "streaming" "Streaming extraction exited with code $STREAM_EXIT"
        exit $STREAM_EXIT
    fi
    
    echo "=== Streaming extraction complete ==="
    write_status "streaming" "100%"
    
    # Cleanup progress file
    rm -f "$PV_PROGRESS_FILE"

# =============================================================================
# Standard Mode (PD) - aria2c download + extract
# =============================================================================
else
    echo "=== Standard mode (aria2c + extract) ==="
    
    # Download with aria2c
    write_status "downloading" "0%"
    echo "=== Downloading snapshot with aria2c ==="
    
    # aria2c with progress logging (using signed URL)
    aria2c \
        --dir="$TEMP_DIR" \
        --out="snapshot.tar.zst" \
        --max-connection-per-server={{ aria2_connections }} \
        --split={{ aria2_split }} \
        --min-split-size={{ aria2_min_split_size }} \
        --file-allocation=none \
        --continue=true \
        --auto-file-renaming=false \
        --allow-overwrite=true \
        --summary-interval=30 \
        --console-log-level=notice \
        "$SIGNED_URL" 2>&1 | tee /tmp/aria2c.log &
    
    ARIA2_PID=$!
    
    # Monitor aria2c progress
    while kill -0 $ARIA2_PID 2>/dev/null; do
        # Parse progress from aria2c log
        if [ -f /tmp/aria2c.log ]; then
            # Look for progress like [#1 SIZE:1.2GiB/2.5GiB(47%)]
            PROGRESS=$(tail -20 /tmp/aria2c.log | grep -oE '\([0-9]+%\)' | tail -1 | tr -d '()' || echo "")
            if [ -n "$PROGRESS" ]; then
                write_status "downloading" "$PROGRESS"
            fi
        fi
        sleep 10
    done
    
    # Wait for aria2c to finish and get exit code
    wait $ARIA2_PID
    ARIA2_EXIT=$?
    
    if [ $ARIA2_EXIT -ne 0 ]; then
        write_status "failed" "download" "aria2c exited with code $ARIA2_EXIT"
        exit $ARIA2_EXIT
    fi
    
    echo "=== Download complete ==="
    write_status "downloading" "100%"
    
    # Extract snapshot
    echo "=== Extracting snapshot ==="
    write_status "extracting" "0%"
    
    # Get file size for progress estimation
    ARCHIVE_SIZE=$(stat -c%s "$SNAPSHOT_FILE" 2>/dev/null || echo "0")
    echo "Archive size: $ARCHIVE_SIZE bytes"
    
    # Extract to temp directory
    cd "$EXTRACT_DIR"
    zstd -d < "$SNAPSHOT_FILE" | tar -xf - --checkpoint=5000 --checkpoint-action=exec='echo "Extracted checkpoint $TAR_CHECKPOINT"' 2>&1 | while read line; do
        echo "$line"
        # Update status periodically
        write_status "extracting" "in progress"
    done
    
    echo "=== Extraction complete ==="
    write_status "extracting" "100%"
    
    # Cleanup downloaded archive to free space
    echo "=== Cleaning up downloaded archive ==="
    rm -rf "$TEMP_DIR"
fi

# =============================================================================
# Common: Fix extraction path and finalize
# =============================================================================
echo "=== Fixing extraction path ==="
write_status "finalizing" "fixing paths"

# Find the actual data directory (look for db/mdbx.dat)
MDBX_PATH=$(find "$EXTRACT_DIR" -name "mdbx.dat" -path "*/db/*" 2>/dev/null | head -1)
if [ -n "$MDBX_PATH" ]; then
    # Get the parent directory of db/ which contains the actual reth data
    ACTUAL_DATA_DIR=$(dirname $(dirname "$MDBX_PATH"))
    echo "Found reth data at: $ACTUAL_DATA_DIR"
    
    # Move contents to the correct location
    if [ "$ACTUAL_DATA_DIR" != "$DATA_DIR" ]; then
        echo "Moving data from $ACTUAL_DATA_DIR to $DATA_DIR"
        # Remove any existing empty directories in DATA_DIR
        rm -rf "$DATA_DIR/db" "$DATA_DIR/static_files" "$DATA_DIR/blobstore" "$DATA_DIR/invalid_block_hooks" 2>/dev/null || true
        # Move the actual data
        mv "$ACTUAL_DATA_DIR"/* "$DATA_DIR/"
    fi
else
    echo "WARNING: Could not find mdbx.dat, assuming direct extraction worked"
    # Move everything from extract temp to data dir
    mv "$EXTRACT_DIR"/* "$DATA_DIR/" 2>/dev/null || true
fi

# Set ownership
chown -R {{ service_user }}:{{ service_group }} "$DATA_DIR"

# Cleanup extraction directory
echo "=== Cleaning up extraction temp ==="
rm -rf "$EXTRACT_DIR"

write_status "complete" "100%"
echo "=== Snapshot download and extraction complete ==="

# Trigger configure-finish service
echo "=== Starting configure-finish service ==="
# Reload systemd in case unit files changed (e.g., by Ansible reconfigure)
systemctl daemon-reload
# Use --no-block to avoid deadlock: configure-finish.service has After=snapshot-download.service,
# so it would wait for this script to exit, but this script waits for systemctl to return.
systemctl start configure-finish.service --no-block
