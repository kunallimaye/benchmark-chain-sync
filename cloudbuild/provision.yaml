# =============================================================================
# Provision Infrastructure (Terraform Only)
# =============================================================================
# Creates/updates benchmark VM(s) from config.toml and generates Ansible inventory.
# Does NOT run Ansible - use configure.yaml for that.
#
# Prerequisites:
#   - L1 infrastructure must exist (run: make create-l1)
#   - Golden snapshot must exist (run: make create-snapshot)
#
# Usage:
#   gcloud builds submit . \
#     --config=cloudbuild/provision.yaml \
#     --substitutions=_L1_API_KEY=...,_VM=op-reth-baseline
#
# =============================================================================

steps:
  # ---------------------------------------------------------------------------
  # Convert config.toml to JSON and merge defaults
  # ---------------------------------------------------------------------------
  - id: 'parse-config'
    name: 'python:3.12-slim'
    entrypoint: 'python3'
    args:
      - '-c'
      - |
        import tomllib
        import json
        import sys

        # Load config.toml
        with open('config.toml', 'rb') as f:
            config = tomllib.load(f)

        # Get defaults for VM, tracing, and reth config
        defaults = config.get('defaults', {}).get('vm', {})
        tracing = config.get('tracing', {})
        snapshot = config.get('snapshot', {})
        reth_config = config.get('reth_config', {})

        # Merge defaults into each VM
        vms = config.get('vm', [])
        for vm in vms:
            # Check if this is an LSSD machine type
            machine_type = vm.get('machine_type', defaults.get('machine_type', ''))
            is_lssd = machine_type.endswith('-lssd')
            
            for key, value in defaults.items():
                if key not in vm:
                    # Don't apply storage_type or disk_size_gb defaults for LSSD machines
                    if is_lssd and key in ('storage_type', 'disk_size_gb'):
                        continue
                    vm[key] = value

        # Filter to specific VM if _VM is set
        vm_filter = '${_VM}'
        if vm_filter:
            vms = [vm for vm in vms if vm.get('name') == vm_filter]
            if not vms:
                print(f"ERROR: VM '{vm_filter}' not found in config.toml", file=sys.stderr)
                sys.exit(1)

        # Update config with merged VMs and ensure configs exist
        config['vm'] = vms
        config['tracing'] = tracing
        config['snapshot'] = snapshot
        config['reth_config'] = reth_config

        # Validate snapshot config
        if not snapshot.get('name'):
            print("ERROR: [snapshot] name is required in config.toml", file=sys.stderr)
            print("Create a golden snapshot first: make create-snapshot VM=<vm-name>", file=sys.stderr)
            sys.exit(1)

        # Write merged config
        with open('/workspace/config.json', 'w') as f:
            json.dump(config, f, indent=2)

        print("=== Parsed config.toml ===")
        print(f"Project: {config.get('project', {}).get('project_id')}")
        print(f"Golden snapshot: {snapshot.get('name')}")
        print(f"VMs to provision: {[vm.get('name') for vm in vms]}")
        print("")
        for vm in vms:
            machine_type = vm.get('machine_type')
            storage_type = vm.get('storage_type')
            is_lssd = machine_type and machine_type.endswith('-lssd')
            confidential = vm.get('confidential_compute', True)
            print(f"  {vm.get('name')}:")
            print(f"    machine_type: {machine_type}")
            print(f"    storage_type: {storage_type or 'lssd (built-in)'}")
            print(f"    confidential_compute: {confidential}")
            if storage_type and storage_type.startswith('hyperdisk'):
                print(f"    provisioned_iops: {vm.get('provisioned_iops')}")
                print(f"    provisioned_throughput: {vm.get('provisioned_throughput')}")
            if not is_lssd and storage_type:
                print(f"    disk_size_gb: {vm.get('disk_size_gb')}")
            print(f"    reth_version: {vm.get('reth_version')}")

        # Write individual values for subsequent steps (avoids jq dependency)
        with open('/workspace/project_id.txt', 'w') as f:
            f.write(config.get('project', {}).get('project_id', ''))
        with open('/workspace/zone.txt', 'w') as f:
            f.write(config.get('project', {}).get('zone', ''))
        with open('/workspace/snapshot_name.txt', 'w') as f:
            f.write(snapshot.get('name', ''))
        with open('/workspace/snapshot_disk_size_gb.txt', 'w') as f:
            f.write(str(snapshot.get('disk_size_gb', 12000)))

  # ---------------------------------------------------------------------------
  # Verify golden snapshot exists
  # ---------------------------------------------------------------------------
  - id: 'verify-snapshot'
    name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        PROJECT_ID=$$(cat /workspace/project_id.txt)
        SNAPSHOT_NAME=$$(cat /workspace/snapshot_name.txt)
        
        echo "Verifying golden snapshot: $$SNAPSHOT_NAME"
        
        if gcloud compute snapshots describe "$$SNAPSHOT_NAME" --project="$$PROJECT_ID" &>/dev/null; then
          STATUS=$$(gcloud compute snapshots describe "$$SNAPSHOT_NAME" --project="$$PROJECT_ID" --format="value(status)")
          echo "Snapshot found, status: $$STATUS"
          if [ "$$STATUS" != "READY" ]; then
            echo "ERROR: Snapshot is not ready (status: $$STATUS)"
            exit 1
          fi
        else
          echo "ERROR: Golden snapshot '$$SNAPSHOT_NAME' not found."
          echo "Create one first: make create-snapshot VM=<vm-name>"
          exit 1
        fi

  # ---------------------------------------------------------------------------
  # Generate Terraform variables from config.json
  # ---------------------------------------------------------------------------
  - id: 'generate-tfvars'
    name: 'python:3.12-slim'
    entrypoint: 'python3'
    args:
      - '-c'
      - |
        import json

        with open('/workspace/config.json', 'r') as f:
            config = json.load(f)

        # Read snapshot config
        snapshot_name = ''
        snapshot_disk_size_gb = 12000
        try:
            with open('/workspace/snapshot_name.txt', 'r') as f:
                snapshot_name = f.read().strip()
            with open('/workspace/snapshot_disk_size_gb.txt', 'r') as f:
                snapshot_disk_size_gb = int(f.read().strip())
        except:
            pass

        project = config.get('project', {})
        l1 = config.get('l1', {})
        vms = config.get('vm', [])
        tracing = config.get('tracing', {})
        snapshot = config.get('snapshot', {})
        reth_config = config.get('reth_config', {})

        # Convert VMs list to map keyed by name
        instances = {}
        for vm in vms:
            name = vm.get('name')
            # storage_type can be None for LSSD machine types
            storage_type = vm.get('storage_type')
            is_hyperdisk = storage_type and storage_type.startswith('hyperdisk')
            
            instances[name] = {
                'machine_type': vm.get('machine_type'),
                'storage_type': storage_type,  # Can be None for LSSD machine types
                'disk_size_gb': vm.get('disk_size_gb', 15000) if storage_type else 0,
                'reth_version': vm.get('reth_version'),
                'op_node_version': vm.get('op_node_version', 'v1.16.5'),
                'confidential_compute': vm.get('confidential_compute', True),
                # Performance tuning
                'engine_cache_mb': vm.get('engine_cache_mb', 4096),
                'engine_workers': vm.get('engine_workers', 0),
                # Node mode: "archive" or "full" (pruned)
                'node_mode': vm.get('node_mode', 'archive'),
                # Hyperdisk settings
                'provisioned_iops': vm.get('provisioned_iops') if is_hyperdisk else None,
                'provisioned_throughput': vm.get('provisioned_throughput') if is_hyperdisk else None,
            }

        # Write terraform.tfvars.json
        tfvars = {
            'project_id': project.get('project_id'),
            'region': project.get('region'),
            'zone': project.get('zone'),
            'network': project.get('network'),
            'gcs_bucket': project.get('gcs_bucket'),
            'l1_rpc_endpoint': l1.get('rpc_endpoint'),
            'l1_beacon_endpoint': l1.get('beacon_endpoint'),
            'l1_api_key': '${_L1_API_KEY}',
            'create_l1': False,
            'snapshot_name': snapshot_name,
            'snapshot_disk_size_gb': snapshot_disk_size_gb,
            'instances': instances,
            'tracing_enabled': tracing.get('enabled', True),
            'tracing_sample_ratio': tracing.get('sample_ratio', 0.01),
            'tracing_filter': tracing.get('filter', 'info'),
            # Reth configuration - Stage thresholds (for Ansible inventory)
            # Defaults are reth's actual defaults for maximum sync performance
            'reth_execution_max_blocks': reth_config.get('execution_max_blocks', 500000),
            'reth_execution_max_changes': reth_config.get('execution_max_changes', 5000000),
            'reth_execution_max_cumulative_gas': reth_config.get('execution_max_cumulative_gas', 1500000000000),
            'reth_execution_max_duration': reth_config.get('execution_max_duration', '10m'),
            'reth_headers_commit_threshold': reth_config.get('headers_commit_threshold', 10000),
            'reth_sender_recovery_commit_threshold': reth_config.get('sender_recovery_commit_threshold', 5000000),
            'reth_account_hashing_clean_threshold': reth_config.get('account_hashing_clean_threshold', 500000),
            'reth_account_hashing_commit_threshold': reth_config.get('account_hashing_commit_threshold', 100000),
            'reth_storage_hashing_clean_threshold': reth_config.get('storage_hashing_clean_threshold', 500000),
            'reth_storage_hashing_commit_threshold': reth_config.get('storage_hashing_commit_threshold', 100000),
            'reth_merkle_incremental_threshold': reth_config.get('merkle_incremental_threshold', 7000),
            'reth_merkle_rebuild_threshold': reth_config.get('merkle_rebuild_threshold', 100000),
            'reth_transaction_lookup_chunk_size': reth_config.get('transaction_lookup_chunk_size', 5000000),
            'reth_index_account_history_commit_threshold': reth_config.get('index_account_history_commit_threshold', 100000),
            'reth_index_storage_history_commit_threshold': reth_config.get('index_storage_history_commit_threshold', 100000),
            'reth_prune_commit_threshold': reth_config.get('prune_commit_threshold', 1000000),
            'reth_etl_file_size': reth_config.get('etl_file_size', 524288000),
            # Database configuration
            'db_max_size_gb': reth_config.get('db_max_size_gb', 15000),
            'db_growth_step_mb': reth_config.get('db_growth_step_mb', 4096),
        }

        with open('/workspace/terraform.tfvars.json', 'w') as f:
            json.dump(tfvars, f, indent=2)

        print("=== Generated terraform.tfvars.json ===")
        # Don't print API key
        tfvars_safe = tfvars.copy()
        tfvars_safe['l1_api_key'] = '***'
        print(json.dumps(tfvars_safe, indent=2))

  # ---------------------------------------------------------------------------
  # Terraform Init
  # ---------------------------------------------------------------------------
  - id: 'terraform-init'
    name: 'hashicorp/terraform:1.9'
    dir: 'terraform'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        terraform init \
          -backend-config="bucket=${_GCS_BUCKET}" \
          -backend-config="prefix=terraform/state/benchmark"

  # ---------------------------------------------------------------------------
  # Terraform Plan/Apply
  # ---------------------------------------------------------------------------
  - id: 'terraform-apply'
    name: 'hashicorp/terraform:1.9'
    dir: 'terraform'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        set -e
        
        # Build target option if specified
        TARGET_OPT=""
        if [ -n "${_TARGET}" ]; then
          TARGET_OPT="-target=${_TARGET}"
          echo "=== Targeting: ${_TARGET} ==="
        fi
        
        if [ "${_PLAN_ONLY}" = "true" ]; then
          echo "=== Terraform Plan (dry-run) ==="
          terraform plan \
            -var-file=/workspace/terraform.tfvars.json \
            $$TARGET_OPT
          echo ""
          echo "=== Plan complete (no changes applied) ==="
        else
          echo "=== Applying Terraform ==="
          terraform apply -auto-approve \
            -var-file=/workspace/terraform.tfvars.json \
            $$TARGET_OPT
          
          echo ""
          echo "=== Terraform Outputs ==="
          terraform output -json
        fi

  # ---------------------------------------------------------------------------
  # Upload inventory and instances.json to GCS (skip in plan-only mode)
  # ---------------------------------------------------------------------------
  - id: 'upload-to-gcs'
    name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        if [ "${_PLAN_ONLY}" = "true" ]; then
          echo "=== Skipping upload (plan-only mode) ==="
          exit 0
        fi
        
        echo "=== Uploading to GCS ==="
        
        # Upload Ansible inventory
        if [ -f ansible/inventory/hosts.yml ]; then
          gsutil cp ansible/inventory/hosts.yml "gs://${_GCS_BUCKET}/ansible/inventory.yml"
          echo "Inventory: gs://${_GCS_BUCKET}/ansible/inventory.yml"
        fi
        
        # Upload config.json for reference
        gsutil cp /workspace/config.json "gs://${_GCS_BUCKET}/terraform/config.json"
        echo "Config: gs://${_GCS_BUCKET}/terraform/config.json"
        
        # Generate and upload instances.json from terraform output
        cd terraform
        terraform output -json instances > /workspace/instances.json 2>/dev/null || echo '{}' > /workspace/instances.json
        gsutil cp /workspace/instances.json "gs://${_GCS_BUCKET}/terraform/instances.json"
        echo "Instances: gs://${_GCS_BUCKET}/terraform/instances.json"
        
        echo ""
        echo "=== Provisioning Complete ==="
        echo "Next step: make configure"

substitutions:
  _GCS_BUCKET: 'base-mainnet-snapshot'
  _L1_API_KEY: ''  # Required: API key for BNE access
  _VM: ''          # Optional: filter to specific VM name
  _PLAN_ONLY: ''   # Optional: set to 'true' for dry-run (plan only, no apply)
  _TARGET: ''      # Optional: terraform -target option (e.g., module.monitoring)

options:
  logging: CLOUD_LOGGING_ONLY

tags: ['provision']

timeout: '1800s'  # 30 minutes
