# =============================================================================
# Configure Instances (Ansible Only)
# =============================================================================
# Downloads inventory from GCS and runs Ansible playbook to configure VMs.
# For LSSD machines: creates a temp disk from snapshot, attaches it, runs
# Ansible (which rsyncs data to LSSD), then detaches and deletes temp disk.
#
# Prerequisites: VMs must be provisioned (run: make provision)
#
# Usage:
#   # Configure single VM
#   gcloud builds submit . \
#     --config=cloudbuild/configure.yaml \
#     --substitutions=_VM=op-reth-baseline
#
#   # Configure all VMs
#   gcloud builds submit . \
#     --config=cloudbuild/configure.yaml
#
# =============================================================================

steps:
  # ---------------------------------------------------------------------------
  # Parse config to get snapshot and LSSD machine info
  # ---------------------------------------------------------------------------
  - id: 'parse-config'
    name: 'python:3.12-slim'
    entrypoint: 'python3'
    args:
      - '-c'
      - |
        import tomllib
        import json

        with open('config.toml', 'rb') as f:
            config = tomllib.load(f)

        snapshot = config.get('snapshot', {})
        project = config.get('project', {})
        defaults = config.get('defaults', {}).get('vm', {})
        vms = config.get('vm', [])

        # Merge defaults into each VM
        for vm in vms:
            machine_type = vm.get('machine_type', defaults.get('machine_type', ''))
            for key, value in defaults.items():
                if key not in vm:
                    if machine_type.endswith('-lssd') and key in ('storage_type', 'disk_size_gb'):
                        continue
                    vm[key] = value

        # Filter to specific VM if _VM is set
        vm_filter = '${_VM}'
        if vm_filter:
            vms = [vm for vm in vms if vm.get('name') == vm_filter]

        # Find LSSD machines that need temp disk
        lssd_vms = [vm for vm in vms if vm.get('machine_type', '').endswith('-lssd')]
        
        print(f"Snapshot: {snapshot.get('name')}")
        print(f"Project: {project.get('project_id')}")
        print(f"Zone: {project.get('zone')}")
        print(f"LSSD VMs that need temp disk: {[vm.get('name') for vm in lssd_vms]}")

        # Write config for subsequent steps
        with open('/workspace/snapshot_name.txt', 'w') as f:
            f.write(snapshot.get('name', ''))
        with open('/workspace/snapshot_disk_size_gb.txt', 'w') as f:
            f.write(str(snapshot.get('disk_size_gb', 12000)))
        with open('/workspace/project_id.txt', 'w') as f:
            f.write(project.get('project_id', ''))
        with open('/workspace/zone.txt', 'w') as f:
            f.write(project.get('zone', ''))
        with open('/workspace/lssd_vms.json', 'w') as f:
            json.dump([{'name': vm.get('name'), 'machine_type': vm.get('machine_type', '')} for vm in lssd_vms], f)

  # ---------------------------------------------------------------------------
  # Download inventory from GCS
  # ---------------------------------------------------------------------------
  - id: 'download-inventory'
    name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        echo "=== Downloading inventory from GCS ==="
        mkdir -p ansible/inventory
        gsutil cp "gs://${_GCS_BUCKET}/ansible/inventory.yml" ansible/inventory/hosts.yml
        
        echo "Inventory contents:"
        cat ansible/inventory/hosts.yml

  # ---------------------------------------------------------------------------
  # Create and attach temp disks for LSSD machines
  # ---------------------------------------------------------------------------
  - id: 'create-lssd-temp-disks'
    name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        PROJECT_ID=$$(cat /workspace/project_id.txt)
        ZONE=$$(cat /workspace/zone.txt)
        SNAPSHOT_NAME=$$(cat /workspace/snapshot_name.txt)
        SNAPSHOT_DISK_SIZE=$$(cat /workspace/snapshot_disk_size_gb.txt)
        LSSD_VMS=$$(cat /workspace/lssd_vms.json)
        
        echo "=== Creating temp disks for LSSD machines ==="
        echo "Snapshot: $$SNAPSHOT_NAME"
        echo "LSSD VMs: $$LSSD_VMS"
        
        # Parse JSON array using python
        python3 -c "
        import json
        import subprocess
        import sys

        lssd_vms = json.loads('$$LSSD_VMS')
        
        for vm in lssd_vms:
            vm_name = vm['name']
            machine_type = vm.get('machine_type', '')
            temp_disk_name = f'{vm_name}-snapshot-temp'
            
            # C4 machines require hyperdisk-balanced, C3 and others can use pd-balanced
            # LSSD machines have built-in local SSDs for data; this temp disk is just
            # for rsync'ing the snapshot data to the local SSDs
            disk_type = 'hyperdisk-balanced' if machine_type.startswith('c4-') else 'pd-balanced'
            
            print(f'Creating temp disk: {temp_disk_name} (type: {disk_type})')
            
            # Check if disk already exists
            result = subprocess.run([
                'gcloud', 'compute', 'disks', 'describe', temp_disk_name,
                '--zone=$$ZONE', '--project=$$PROJECT_ID'
            ], capture_output=True)
            
            if result.returncode == 0:
                print(f'  Temp disk already exists, skipping creation')
            else:
                # Create disk from snapshot
                subprocess.run([
                    'gcloud', 'compute', 'disks', 'create', temp_disk_name,
                    '--source-snapshot=$$SNAPSHOT_NAME',
                    f'--type={disk_type}',
                    '--size=$${SNAPSHOT_DISK_SIZE}GB',
                    '--zone=$$ZONE',
                    '--project=$$PROJECT_ID'
                ], check=True)
                print(f'  Created temp disk: {temp_disk_name}')
            
            # Attach disk to VM
            # Note: hyperdisk-balanced doesn't support read-only mode, so we attach in rw mode
            # The rsync will only read from it, and we delete it after anyway
            print(f'Attaching temp disk to {vm_name}')
            result = subprocess.run([
                'gcloud', 'compute', 'instances', 'attach-disk', vm_name,
                '--disk=' + temp_disk_name,
                '--device-name=snapshot-disk',
                '--zone=$$ZONE',
                '--project=$$PROJECT_ID'
            ], capture_output=True)
            
            if result.returncode == 0:
                print(f'  Attached temp disk to {vm_name}')
            else:
                # May already be attached
                print(f'  Note: {result.stderr.decode()}')
        
        print('Done creating and attaching temp disks')
        "

  # ---------------------------------------------------------------------------
  # Setup SSH keys via gcloud for OS Login
  # ---------------------------------------------------------------------------
  - id: 'setup-ssh'
    name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        echo "=== Setting up SSH keys for OS Login ==="
        
        # Clean up old SSH keys to avoid profile size limit (32 KiB)
        echo "Cleaning up old SSH keys..."
        OLD_KEYS=$$(gcloud compute os-login describe-profile --format=json 2>/dev/null | \
          python3 -c "import sys,json; d=json.load(sys.stdin); keys=d.get('sshPublicKeys',{}); print(' '.join(list(keys.keys())[:-3]) if len(keys)>3 else '')" 2>/dev/null || echo "")
        
        for KEY_FP in $$OLD_KEYS; do
          echo "  Removing old key: $$KEY_FP"
          gcloud compute os-login ssh-keys remove --key=$$KEY_FP 2>/dev/null || true
        done
        
        # Generate SSH key pair if not exists
        mkdir -p /root/.ssh
        if [ ! -f /root/.ssh/google_compute_engine ]; then
          ssh-keygen -t rsa -b 4096 -f /root/.ssh/google_compute_engine -N "" -q
        fi
        
        # Add SSH key to OS Login
        gcloud compute os-login ssh-keys add --key-file=/root/.ssh/google_compute_engine.pub --project=${_PROJECT_ID}
        
        # Get the OS Login username
        OSLOGIN_USER=$$(gcloud compute os-login describe-profile --format='value(posixAccounts[0].username)')
        echo "OS Login user: $$OSLOGIN_USER"
        echo "$$OSLOGIN_USER" > /workspace/oslogin_user.txt
        
        # Copy keys to workspace for next step
        cp /root/.ssh/google_compute_engine /workspace/
        cp /root/.ssh/google_compute_engine.pub /workspace/
        chmod 600 /workspace/google_compute_engine
        
        echo "SSH keys configured"

  # ---------------------------------------------------------------------------
  # Run Ansible Playbook
  # ---------------------------------------------------------------------------
  - id: 'ansible-configure'
    name: 'cytopia/ansible:latest-tools'
    dir: 'ansible'
    entrypoint: '/bin/sh'
    env:
      - 'ANSIBLE_HOST_KEY_CHECKING=False'
      - 'ANSIBLE_STRATEGY=free'
    args:
      - '-c'
      - |
        set -e
        
        echo "=== Running Ansible Playbook ==="
        
        # Get OS Login username from previous step
        OSLOGIN_USER=$$(cat /workspace/oslogin_user.txt)
        echo "Using OS Login user: $$OSLOGIN_USER"
        
        # Setup SSH key
        mkdir -p /root/.ssh
        cp /workspace/google_compute_engine /root/.ssh/
        cp /workspace/google_compute_engine.pub /root/.ssh/
        chmod 600 /root/.ssh/google_compute_engine
        
        # Determine limit based on parameters
        if [ -n "${_VM}" ]; then
          echo "Configuring VM: ${_VM}"
          LIMIT="--limit=${_VM}"
        else
          echo "Configuring ALL VMs"
          LIMIT=""
        fi
        
        ansible-playbook \
          -i inventory/hosts.yml \
          playbooks/site.yml \
          --forks=${_FORKS} \
          --user=$$OSLOGIN_USER \
          --private-key=/root/.ssh/google_compute_engine \
          --become \
          $$LIMIT \
          -v
        
        echo ""
        echo "=== Configuration Complete ==="

  # ---------------------------------------------------------------------------
  # Detach and delete temp disks for LSSD machines
  # ---------------------------------------------------------------------------
  - id: 'cleanup-lssd-temp-disks'
    name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        PROJECT_ID=$$(cat /workspace/project_id.txt)
        ZONE=$$(cat /workspace/zone.txt)
        LSSD_VMS=$$(cat /workspace/lssd_vms.json)
        
        echo "=== Cleaning up temp disks for LSSD machines ==="
        
        python3 -c "
        import json
        import subprocess

        lssd_vms = json.loads('$$LSSD_VMS')
        
        for vm in lssd_vms:
            vm_name = vm['name'] if isinstance(vm, dict) else vm
            temp_disk_name = f'{vm_name}-snapshot-temp'
            
            # Detach disk from VM
            print(f'Detaching temp disk from {vm_name}')
            subprocess.run([
                'gcloud', 'compute', 'instances', 'detach-disk', vm_name,
                '--disk=' + temp_disk_name,
                '--zone=$$ZONE',
                '--project=$$PROJECT_ID'
            ], capture_output=True)
            
            # Delete temp disk
            print(f'Deleting temp disk: {temp_disk_name}')
            subprocess.run([
                'gcloud', 'compute', 'disks', 'delete', temp_disk_name,
                '--zone=$$ZONE',
                '--project=$$PROJECT_ID',
                '--quiet'
            ], capture_output=True)
            
            print(f'  Cleaned up {temp_disk_name}')
        
        print('Done cleaning up temp disks')
        "

substitutions:
  _VM: ''
  _FORKS: '10'
  _GCS_BUCKET: 'base-mainnet-snapshot'
  _PROJECT_ID: 'bct-prod-c3-tdx-3'

options:
  logging: CLOUD_LOGGING_ONLY
  pool:
    name: 'projects/bct-prod-c3-tdx-3/locations/us-central1/workerPools/benchmark-build-pool'

tags: ['configure']

timeout: '21600s'  # 6 hours (large database operations can take time)
