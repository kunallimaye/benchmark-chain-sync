# =============================================================================
# Provision Benchmark VMs (Terraform Only)
# =============================================================================
# Creates/updates benchmark VM(s) from config.toml.
# Uses the new modular terraform structure at terraform/benchmark/
#
# Prerequisites:
#   - Foundation must be applied (terraform/foundation/)
#   - Golden snapshot must exist (run: make create-snapshot)
#
# Usage:
#   gcloud builds submit . \
#     --config=cloudbuild/benchmark/provision.yaml \
#     --substitutions=_VM=op-reth-baseline
#
# =============================================================================

steps:
  # ---------------------------------------------------------------------------
  # Convert config.toml to JSON and generate vms.json for Terraform
  # ---------------------------------------------------------------------------
  - id: 'parse-config'
    name: 'python:3.12-slim'
    entrypoint: 'python3'
    args:
      - '-c'
      - |
        import tomllib
        import json
        import sys

        # Load config.toml
        with open('config.toml', 'rb') as f:
            config = tomllib.load(f)

        # Get defaults for VM, tracing, and reth config
        defaults = config.get('defaults', {}).get('vm', {})
        tracing = config.get('tracing', {})
        snapshot = config.get('snapshot', {})
        reth_config = config.get('reth_config', {})
        project = config.get('project', {})

        # Merge defaults into each VM
        vms = config.get('vm', [])
        for vm in vms:
            for key, value in defaults.items():
                if key not in vm:
                    vm[key] = value
            
            # Check if this uses built-in local SSD storage
            has_builtin_lssd = vm.get('storage_type') == 'inbuilt-lssd'
            
            # Don't apply disk_size_gb for inbuilt-lssd (uses built-in NVMe)
            if has_builtin_lssd and 'disk_size_gb' in vm and vm.get('disk_size_gb') == defaults.get('disk_size_gb'):
                del vm['disk_size_gb']

        # Filter to specific VM if _VM is set
        vm_filter = '${_VM}'
        if vm_filter:
            vms = [vm for vm in vms if vm.get('name') == vm_filter]
            if not vms:
                print(f"ERROR: VM '{vm_filter}' not found in config.toml", file=sys.stderr)
                sys.exit(1)

        # Validate snapshot config
        if not snapshot.get('name'):
            print("ERROR: [snapshot] name is required in config.toml", file=sys.stderr)
            print("Create a golden snapshot first: make create-snapshot VM=<vm-name>", file=sys.stderr)
            sys.exit(1)

        print("=== Parsed config.toml ===")
        print(f"Project: {project.get('project_id')}")
        print(f"Golden snapshot: {snapshot.get('name')}")
        print(f"VMs to provision: {[vm.get('name') for vm in vms]}")
        print("")
        for vm in vms:
            machine_type = vm.get('machine_type')
            storage_type = vm.get('storage_type')
            has_builtin_lssd = storage_type == 'inbuilt-lssd'
            confidential = vm.get('confidential_compute', True)
            print(f"  {vm.get('name')}:")
            print(f"    machine_type: {machine_type}")
            print(f"    storage_type: {storage_type}")
            print(f"    confidential_compute: {confidential}")
            if storage_type and storage_type.startswith('hyperdisk'):
                print(f"    provisioned_iops: {vm.get('provisioned_iops')}")
                print(f"    provisioned_throughput: {vm.get('provisioned_throughput')}")
            if not has_builtin_lssd:
                print(f"    disk_size_gb: {vm.get('disk_size_gb')}")
            print(f"    reth_version: {vm.get('reth_version')}")

        # Write vms.json for Terraform benchmark module
        vms_config = []
        for vm in vms:
            vms_config.append({
                'name': vm.get('name'),
                'zone': vm.get('zone', project.get('zone')),
                'machine_type': vm.get('machine_type'),
                'storage_type': vm.get('storage_type'),
                'disk_size_gb': vm.get('disk_size_gb', 15000),
                'provisioned_iops': vm.get('provisioned_iops'),
                'provisioned_throughput': vm.get('provisioned_throughput'),
                'confidential_compute': vm.get('confidential_compute', False),
            })
        
        with open('terraform/benchmark/vms.json', 'w') as f:
            json.dump(vms_config, f, indent=2)
        
        print("\n=== Generated terraform/benchmark/vms.json ===")

        # Write individual values for subsequent steps
        with open('/workspace/project_id.txt', 'w') as f:
            f.write(project.get('project_id', ''))
        with open('/workspace/region.txt', 'w') as f:
            f.write(project.get('region', 'us-central1'))
        with open('/workspace/network.txt', 'w') as f:
            f.write(project.get('network', 'default'))
        with open('/workspace/snapshot_name.txt', 'w') as f:
            f.write(snapshot.get('name', ''))
        with open('/workspace/snapshot_disk_size_gb.txt', 'w') as f:
            f.write(str(snapshot.get('disk_size_gb', 12000)))
        with open('/workspace/reth_version.txt', 'w') as f:
            # Use first VM's reth_version or default
            f.write(vms[0].get('reth_version', 'main') if vms else 'main')

        # Store full config for reference
        config['vm'] = vms
        with open('/workspace/config.json', 'w') as f:
            json.dump(config, f, indent=2)

  # ---------------------------------------------------------------------------
  # Verify golden snapshot exists
  # ---------------------------------------------------------------------------
  - id: 'verify-snapshot'
    name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        PROJECT_ID=$$(cat /workspace/project_id.txt)
        SNAPSHOT_NAME=$$(cat /workspace/snapshot_name.txt)
        
        echo "Verifying golden snapshot: $$SNAPSHOT_NAME"
        
        if gcloud compute snapshots describe "$$SNAPSHOT_NAME" --project="$$PROJECT_ID" &>/dev/null; then
          STATUS=$$(gcloud compute snapshots describe "$$SNAPSHOT_NAME" --project="$$PROJECT_ID" --format="value(status)")
          echo "Snapshot found, status: $$STATUS"
          if [ "$$STATUS" != "READY" ]; then
            echo "ERROR: Snapshot is not ready (status: $$STATUS)"
            exit 1
          fi
        else
          echo "ERROR: Golden snapshot '$$SNAPSHOT_NAME' not found."
          echo "Create one first: make create-snapshot VM=<vm-name>"
          exit 1
        fi

  # ---------------------------------------------------------------------------
  # Get service account email from foundation state
  # ---------------------------------------------------------------------------
  - id: 'get-foundation-outputs'
    name: 'hashicorp/terraform:1.9'
    dir: 'terraform/foundation'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        terraform init -input=false
        terraform output -raw service_account_email > /workspace/service_account_email.txt || echo ""
        echo "Service account: $$(cat /workspace/service_account_email.txt)"

  # ---------------------------------------------------------------------------
  # Terraform Init for benchmark module
  # ---------------------------------------------------------------------------
  - id: 'terraform-init'
    name: 'hashicorp/terraform:1.9'
    dir: 'terraform/benchmark'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        terraform init -input=false

  # ---------------------------------------------------------------------------
  # Terraform Plan/Apply with Replacement Detection
  # ---------------------------------------------------------------------------
  # Always runs plan first to detect destructive changes.
  # If any resources would be replaced (deleted+created), blocks unless FORCE=true.
  # This protects against accidental VM recreation which causes data loss on LSSD.
  # ---------------------------------------------------------------------------
  - id: 'terraform-apply'
    name: 'hashicorp/terraform:1.9'
    dir: 'terraform/benchmark'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        set -e
        
        # Read config values
        PROJECT_ID=$$(cat /workspace/project_id.txt)
        REGION=$$(cat /workspace/region.txt)
        NETWORK=$$(cat /workspace/network.txt)
        SNAPSHOT_NAME=$$(cat /workspace/snapshot_name.txt)
        SNAPSHOT_DISK_SIZE_GB=$$(cat /workspace/snapshot_disk_size_gb.txt)
        RETH_VERSION=$$(cat /workspace/reth_version.txt)
        SERVICE_ACCOUNT=$$(cat /workspace/service_account_email.txt)
        
        # If no service account from foundation, use default compute SA
        if [ -z "$$SERVICE_ACCOUNT" ]; then
          PROJECT_NUMBER=$$(gcloud projects describe $$PROJECT_ID --format="value(projectNumber)")
          SERVICE_ACCOUNT="$${PROJECT_NUMBER}-compute@developer.gserviceaccount.com"
          echo "Using default compute service account: $$SERVICE_ACCOUNT"
        fi
        
        # Build common TF vars
        TF_VARS="-var=project_id=$$PROJECT_ID -var=region=$$REGION -var=network=$$NETWORK"
        TF_VARS="$$TF_VARS -var=snapshot_name=$$SNAPSHOT_NAME -var=snapshot_disk_size_gb=$$SNAPSHOT_DISK_SIZE_GB"
        TF_VARS="$$TF_VARS -var=reth_version=$$RETH_VERSION -var=service_account_email=$$SERVICE_ACCOUNT"
        
        echo "=== Running Terraform Plan ==="
        terraform plan -out=tfplan $$TF_VARS
        
        # Check for replacements (destructive changes)
        echo ""
        echo "=== Checking for destructive changes ==="
        terraform show -json tfplan > plan.json
        
        # Use jq to find resources with both 'delete' and 'create' actions (replacements)
        # jq is available in the terraform image
        REPLACEMENTS=$$(cat plan.json | jq -r '
          .resource_changes[]? |
          select(.change.actions | (contains(["delete"]) and contains(["create"]))) |
          .address
        ' 2>/dev/null || echo "")
        
        if [ -n "$$REPLACEMENTS" ]; then
          echo ""
          echo "============================================================"
          echo "ERROR: The following resources would be REPLACED:"
          echo "============================================================"
          echo "$$REPLACEMENTS"
          echo ""
          echo "Replacing VMs will cause:"
          echo "  - VM downtime"
          echo "  - Data loss on LSSD machines (must re-rsync from snapshot)"
          echo ""
          
          if [ "${_FORCE}" != "true" ]; then
            echo "To proceed, run: make provision FORCE=true"
            echo "============================================================"
            exit 1
          else
            echo "FORCE=true specified - proceeding with replacement"
            echo "============================================================"
          fi
        else
          echo "No destructive changes detected."
        fi
        
        if [ "${_PLAN_ONLY}" = "true" ]; then
          echo ""
          echo "=== Plan complete (no changes applied) ==="
          exit 0
        fi
        
        echo ""
        echo "=== Applying Terraform ==="
        terraform apply tfplan
        
        echo ""
        echo "=== Terraform Outputs ==="
        terraform output -json

  # ---------------------------------------------------------------------------
  # Generate Ansible inventory from config.json and VM IPs
  # ---------------------------------------------------------------------------
  - id: 'generate-inventory'
    name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        if [ "${_PLAN_ONLY}" = "true" ]; then
          echo "=== Skipping inventory generation (plan-only mode) ==="
          exit 0
        fi
        
        echo "=== Generating Ansible Inventory ==="
        
        # Read values from files created by parse-config step
        PROJECT_ID=$$(cat /workspace/project_id.txt)
        DEFAULT_ZONE=$$(cat terraform/benchmark/vms.json | python3 -c "import sys,json; vms=json.load(sys.stdin); print(vms[0]['zone'] if vms else 'us-central1-a')")
        
        # Get VM IPs and build inventory using jq and gcloud
        # Read config.json created by parse-config step
        CONFIG_JSON=$$(cat /workspace/config.json)
        
        # Extract values from config
        GCS_BUCKET=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('project',{}).get('gcs_bucket',''))")
        NETWORK=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('project',{}).get('chain_network','base-mainnet'))")
        L1_RPC=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('l1',{}).get('rpc_endpoint',''))")
        L1_BEACON=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('l1',{}).get('beacon_endpoint',''))")
        TRACING_ENABLED=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(str(c.get('tracing',{}).get('enabled',False)).lower())")
        TRACING_RATIO=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('tracing',{}).get('sample_ratio',0.01))")
        TRACING_FILTER=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('tracing',{}).get('filter','info'))")
        DB_MAX_SIZE_GB=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('reth_config',{}).get('db_max_size_gb',15000))")
        DB_GROWTH_MB=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('reth_config',{}).get('db_growth_step_mb',4096))")
        OP_NODE_VERSION=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(c.get('build',{}).get('op_node_version','v1.16.5'))")
        
        # Calculate byte values
        DB_MAX_SIZE_BYTES=$$((DB_MAX_SIZE_GB * 1024 * 1024 * 1024))
        DB_GROWTH_BYTES=$$((DB_GROWTH_MB * 1024 * 1024))
        
        # Get defaults
        DEFAULTS=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(json.dumps(c.get('defaults',{}).get('vm',{})))")
        
        # Build hosts JSON
        echo "Building hosts inventory..."
        HOSTS_JSON="{}"
        
        # Get VM list from config.json (already has defaults merged)
        VMS=$$(echo "$$CONFIG_JSON" | python3 -c "import sys,json; c=json.load(sys.stdin); print(json.dumps(c.get('vm',[])))")
        VM_COUNT=$$(echo "$$VMS" | python3 -c "import sys,json; print(len(json.load(sys.stdin)))")
        
        for i in $$(seq 0 $$((VM_COUNT - 1))); do
          VM=$$(echo "$$VMS" | python3 -c "import sys,json; vms=json.load(sys.stdin); print(json.dumps(vms[$$i]))")
          VM_NAME=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('name',''))")
          MACHINE_TYPE=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('machine_type',''))")
          STORAGE_TYPE=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('storage_type','pd-balanced'))")
          IS_LSSD=$$([ "$$STORAGE_TYPE" = "inbuilt-lssd" ] && echo "true" || echo "false")
          CONFIDENTIAL=$$(echo "$$VM" | python3 -c "import sys,json; print(str(json.load(sys.stdin).get('confidential_compute',True)).lower())")
          ENGINE_CACHE=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('engine_cache_mb',16384))")
          ENGINE_WORKERS=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('engine_workers',44))")
          RETH_VERSION=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('reth_version','main'))")
          NODE_MODE=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('node_mode','full'))")
          VM_ZONE=$$(echo "$$VM" | python3 -c "import sys,json; print(json.load(sys.stdin).get('zone','$$DEFAULT_ZONE'))")
          
          # Get VM IP from gcloud (using per-VM zone)
          VM_IP=$$(gcloud compute instances describe "$$VM_NAME" \
            --project="$$PROJECT_ID" \
            --zone="$$VM_ZONE" \
            --format="value(networkInterfaces[0].networkIP)" 2>/dev/null || echo "")
          
          if [ -z "$$VM_IP" ]; then
            echo "Warning: Could not get IP for $$VM_NAME, skipping"
            continue
          fi
          
          echo "  - $$VM_NAME: $$VM_IP (zone: $$VM_ZONE)"
          
          # Use lssd as display storage_type for inbuilt-lssd
          DISPLAY_STORAGE_TYPE="$$STORAGE_TYPE"
          if [ "$$IS_LSSD" = "true" ]; then
            DISPLAY_STORAGE_TYPE="lssd"
          fi
          
          # Add host entry
          HOST_ENTRY=$$(cat <<EOF
        {
          "ansible_host": "$$VM_IP",
          "ansible_ssh_common_args": "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null",
          "vm_name": "$$VM_NAME",
          "zone": "$$VM_ZONE",
          "machine_type": "$$MACHINE_TYPE",
          "storage_type": "$$DISPLAY_STORAGE_TYPE",
          "is_lssd_machine": $$IS_LSSD,
          "confidential_compute": $$CONFIDENTIAL,
          "engine_cache_mb": $$ENGINE_CACHE,
          "engine_workers": $$ENGINE_WORKERS,
          "reth_version": "$$RETH_VERSION",
          "op_node_version": "$$OP_NODE_VERSION",
          "node_mode": "$$NODE_MODE"
        }
        EOF
        )
          HOSTS_JSON=$$(echo "$$HOSTS_JSON" | python3 -c "import sys,json; h=json.load(sys.stdin); h['$$VM_NAME']=json.loads('''$$HOST_ENTRY'''); print(json.dumps(h))")
        done
        
        # Build final inventory
        cat > /workspace/inventory.json <<EOF
        {
          "all": {
            "hosts": $$HOSTS_JSON,
            "vars": {
              "gcs_bucket": "$$GCS_BUCKET",
              "network": "$$NETWORK",
              "l1_rpc_endpoint": "$$L1_RPC",
              "l1_beacon_endpoint": "$$L1_BEACON",
              "tracing_enabled": $$TRACING_ENABLED,
              "tracing_sample_ratio": $$TRACING_RATIO,
              "tracing_filter": "$$TRACING_FILTER",
              "db_max_size_bytes": $$DB_MAX_SIZE_BYTES,
              "db_growth_step_bytes": $$DB_GROWTH_BYTES,
              "l1_api_key": "${_L1_API_KEY}"
            }
          }
        }
        EOF
        
        echo ""
        echo "Generated inventory:"
        cat /workspace/inventory.json | python3 -c "import sys,json; inv=json.load(sys.stdin); print(f\"  Hosts: {len(inv['all']['hosts'])}\")"

  # ---------------------------------------------------------------------------
  # Upload config and inventory to GCS (skip in plan-only mode)
  # ---------------------------------------------------------------------------
  - id: 'upload-to-gcs'
    name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        
        if [ "${_PLAN_ONLY}" = "true" ]; then
          echo "=== Skipping upload (plan-only mode) ==="
          exit 0
        fi
        
        echo "=== Uploading to GCS ==="
        
        # Upload config.json for reference
        gsutil cp /workspace/config.json "gs://${_GCS_BUCKET}/terraform/config.json"
        echo "Config: gs://${_GCS_BUCKET}/terraform/config.json"
        
        # Upload vms.json
        gsutil cp terraform/benchmark/vms.json "gs://${_GCS_BUCKET}/terraform/vms.json"
        echo "VMs: gs://${_GCS_BUCKET}/terraform/vms.json"
        
        # Upload Ansible inventory
        gsutil cp /workspace/inventory.json "gs://${_GCS_BUCKET}/ansible/inventory.json"
        echo "Inventory: gs://${_GCS_BUCKET}/ansible/inventory.json"
        
        echo ""
        echo "=== Provisioning Complete ==="
        echo "Next step: make configure"

substitutions:
  _GCS_BUCKET: 'base-mainnet-snapshot'
  _VM: ''          # Optional: filter to specific VM name
  _PLAN_ONLY: ''   # Optional: set to 'true' for dry-run (plan only, no apply)
  _FORCE: ''       # Optional: set to 'true' to allow VM recreation
  _L1_API_KEY: ''  # Required: API key for BNE access (from .env)

options:
  logging: CLOUD_LOGGING_ONLY

tags: ['provision', 'benchmark']

timeout: '1800s'  # 30 minutes
